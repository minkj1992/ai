{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8824d9",
   "metadata": {},
   "source": [
    "# 1. GPT 모델\n",
    "\n",
    "기존의 encoder-decoder 구조인 Transformer 모델을 변형하여 GPT 모델 구조를 만들기 위해서는 아래와 같은 변경이 필요합니다. 이때 빨간색 박스는 사라져야할 컴포넌트이며, 초록색 박스는 변경되어야 하는 컴포넌트를 의미합니다.\n",
    "![](https://github.com/minkj1992/ai/blob/main/static/Untitled-2024-06-21-1948.png?raw=true)\n",
    "\n",
    "openai에서 발표한 [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 논문에 따르면, decoder 모델을 토대로 generalized한 pre-trained 모델을 만듭니다. 그러므로 transformer의 사라져야할 부분은 아래와 같습니다.\n",
    "\n",
    "#### 사라져야할 부분\n",
    "> 빨간 네모\n",
    "1. `Encoder`\n",
    "2. `encoder-decoder attention layer`\n",
    "\n",
    "\n",
    "위 논문에 따르면 변경되어야할 부분은 아래와 같습니다.\n",
    "\n",
    "#### 변경되어야 할 부분\n",
    "> 초록 네모\n",
    "\n",
    "1. Positional encoding -> Positional embedding\n",
    "2. Target input -> `Task-specific input transformations`\n",
    "\n",
    "해당 논문에서는 또한 기존의 sin,cos을 활용한 positional 계산 방식을 positional embedding (max_sentence_len, d_model)을 생성하여 학습되도록 합니다. \n",
    "\n",
    "Target input 또한 달라져야 하는데, 이번 시간에는 pretrain + fine tunning 총 2번 학습했던 논문과 달리 1번만 학습할 예정이기 때문에, input을 변형해서 넣어주도록 하였습니다. \n",
    "\n",
    "현재 데이터 셋은 사용자의 질문인, df['Q']와 챗봇의 답변인 df['A']가 존재하며, 이 두 데이터 모두 챗봇에 필요하며 부족한 데이터를 고려하여, 합쳐서 학습해주도록 하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcbf9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c276db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>역시 못잊겠습니다.</td>\n",
       "      <td>영원한 아픔은 없을 거예요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10048</th>\n",
       "      <td>서로 사랑하면 다 될 줄 알았어</td>\n",
       "      <td>살아가는데 사랑이 큰 힘이 되지만, 모든걸 다 해결해주지는 않아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7724</th>\n",
       "      <td>이제는 잡을 수가 없네</td>\n",
       "      <td>마지막을 아는 것도 필요해요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>내일 수학여행가!</td>\n",
       "      <td>친구들과 좋은 추억 만들고 오세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>당신도 저만큼 아팠으면 좋겠습니다.</td>\n",
       "      <td>이런 말을 하면서 당신이 아프지 않았으면 좋겠어요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>몸이 여러 개면 좋겠다</td>\n",
       "      <td>그러면 못할 게 없겠네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>무리에 잘못 낀 듯</td>\n",
       "      <td>지금도 늦지 않았어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4693</th>\n",
       "      <td>카드 다 부러뜨려 버려야겠다.</td>\n",
       "      <td>소비 조절을 못하면 없애는 것도 방법이 될 거예요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>손님 오는 거 싫어</td>\n",
       "      <td>밖에서 만나보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11768</th>\n",
       "      <td>학생일 때 썸 괜찮을까</td>\n",
       "      <td>지금은 지나면 돌아오지 않아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>놀고 싶다.</td>\n",
       "      <td>저도요!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>7년이라는 세월 함께한 애인과 이별하게 되었습니다.</td>\n",
       "      <td>세월 만큼 힘들겠네요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>자기애가 너무 떨어졌어</td>\n",
       "      <td>매일 거울을 보며 스스로 칭찬해보세요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>비 온다</td>\n",
       "      <td>비 맞으면 안돼요!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3660</th>\n",
       "      <td>이 좀 여유롭게 살고 싶다</td>\n",
       "      <td>여유는 마음 가짐에 있어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Q                                      A  \\\n",
       "7078                     역시 못잊겠습니다.                        영원한 아픔은 없을 거예요.   \n",
       "10048             서로 사랑하면 다 될 줄 알았어  살아가는데 사랑이 큰 힘이 되지만, 모든걸 다 해결해주지는 않아요.   \n",
       "7724                   이제는 잡을 수가 없네                       마지막을 아는 것도 필요해요.   \n",
       "899                       내일 수학여행가!                    친구들과 좋은 추억 만들고 오세요.   \n",
       "6085            당신도 저만큼 아팠으면 좋겠습니다.           이런 말을 하면서 당신이 아프지 않았으면 좋겠어요.   \n",
       "1741                   몸이 여러 개면 좋겠다                         그러면 못할 게 없겠네요.   \n",
       "1757                     무리에 잘못 낀 듯                           지금도 늦지 않았어요.   \n",
       "4693               카드 다 부러뜨려 버려야겠다.           소비 조절을 못하면 없애는 것도 방법이 될 거예요.   \n",
       "2548                     손님 오는 거 싫어                             밖에서 만나보세요.   \n",
       "11768                  학생일 때 썸 괜찮을까                      지금은 지나면 돌아오지 않아요.   \n",
       "1019                         놀고 싶다.                                  저도요!!   \n",
       "5406   7년이라는 세월 함께한 애인과 이별하게 되었습니다.                           세월 만큼 힘들겠네요.   \n",
       "3899                   자기애가 너무 떨어졌어                  매일 거울을 보며 스스로 칭찬해보세요.   \n",
       "2199                           비 온다                             비 맞으면 안돼요!   \n",
       "3660                 이 좀 여유롭게 살고 싶다                        여유는 마음 가짐에 있어요.   \n",
       "\n",
       "       label  \n",
       "7078       1  \n",
       "10048      2  \n",
       "7724       1  \n",
       "899        0  \n",
       "6085       1  \n",
       "1741       0  \n",
       "1757       0  \n",
       "4693       0  \n",
       "2548       0  \n",
       "11768      2  \n",
       "1019       0  \n",
       "5406       1  \n",
       "3899       0  \n",
       "2199       0  \n",
       "3660       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/ChatbotData .csv' \n",
    "origin = pd.read_csv(path) \n",
    "\n",
    "print(origin.shape)\n",
    "origin.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46c937",
   "metadata": {},
   "source": [
    "# 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0604e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23646, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12479</th>\n",
       "      <td>진짜 하고 싶은 걸 찾아보세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>사업 구상하고 있어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13049</th>\n",
       "      <td>행복은 마음 가짐에 있어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>긴 머리가 잘 어울렸던 그녀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>레시피대로 한 거 같은데 왜 맛이 없지?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>전여친이 나보다 나은 사람이라는 생각이 자꾸 들어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14879</th>\n",
       "      <td>그냥 잊어버리세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15571</th>\n",
       "      <td>벌써 그러면 안돼요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8002</th>\n",
       "      <td>전여친한테 남친이 생겼네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16484</th>\n",
       "      <td>당신은 생각보다 큰 사람이에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>워워~ 진정하세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21861</th>\n",
       "      <td>찔러보는 상대 별로네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>내 마음에 마지막 인사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10080</th>\n",
       "      <td>소개팅하고 애프터까지 왔는데 너무 어색해.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22360</th>\n",
       "      <td>사람 마다 다르겠지요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text\n",
       "12479            진짜 하고 싶은 걸 찾아보세요.\n",
       "2290                    사업 구상하고 있어\n",
       "13049              행복은 마음 가짐에 있어요.\n",
       "9103               긴 머리가 잘 어울렸던 그녀\n",
       "1436        레시피대로 한 거 같은데 왜 맛이 없지?\n",
       "8000   전여친이 나보다 나은 사람이라는 생각이 자꾸 들어\n",
       "14879                   그냥 잊어버리세요.\n",
       "15571                  벌써 그러면 안돼요.\n",
       "8002                 전여친한테 남친이 생겼네\n",
       "16484            당신은 생각보다 큰 사람이에요.\n",
       "17303                   워워~ 진정하세요.\n",
       "21861                찔러보는 상대 별로네요.\n",
       "5853                  내 마음에 마지막 인사\n",
       "10080      소개팅하고 애프터까지 왔는데 너무 어색해.\n",
       "22360                 사람 마다 다르겠지요."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = origin.copy()\n",
    "\n",
    "df_q = pd.DataFrame({\n",
    "    'text': tmp['Q']\n",
    "})\n",
    "df_a = pd.DataFrame({\n",
    "    'text': tmp['A']\n",
    "})\n",
    "\n",
    "df = pd.concat([df_q, df_a], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910c9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19436, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9126cc442f14f3c83fc5cb0da7f574d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19413, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def postprocess(df):\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    df.to_csv('./gpt.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.drop_duplicates(subset = ['text'], inplace = True)\n",
    "df['text'] = df['text'].progress_apply(preprocess)\n",
    "postprocess(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3f8cd",
   "metadata": {},
   "source": [
    "## 2. Data prepare\n",
    "\n",
    "- 질문,답변 각자 적절한 maximum 크기만 남기고 지운다.\n",
    "- 모두 하나의 col으로 합쳐서 train한다. (Q, A말투 모두 학습)\n",
    "\n",
    "\n",
    "## (WIP) 추후 도입 방법\n",
    "1. Genralized Pretrain\n",
    "2. Fine tunning\n",
    "\n",
    "두가지를 수행하기 위해서 2가지에 필요한 데이터를 준비하겠습니다.\n",
    "\n",
    "1번을 위해서는 하나의 row에 속한 df['Q']와 df['A']를 2개의 row로 나눠서 df['text']로 만들어 0...i -> i+1예측 teacher forcing을 합니다.\n",
    "\n",
    "이후 2번은 `[SOS] Q [DELIM] A_set [EOS]`형태의 문장을 A_set만큼 만들어, 정답인 A를 예측하도록 하는 softmax를 생성, 이를 위해서는 정답이 아닌 negative sampling이 필요해서, 다음 기회에 시도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fe88e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text의 최소 길이 : 1\n",
      "text의 최대 길이 : 24\n",
      "text의 평균 길이 : 4.318085818781229\n",
      "전체 샘플 중 길이가 9 이하인 샘플의 비율: 0.9841858548395405\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if len(s.split()) <= max_len:\n",
    "            cnt = cnt + 1\n",
    "    print(\n",
    "        \"전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s\"\n",
    "        % (max_len, (cnt / len(nested_list)))\n",
    "    )\n",
    "\n",
    "    \n",
    "len_text = [len(s.split()) for s in df['text']]\n",
    "max_len_text = 9\n",
    "print(\"text의 최소 길이 : {}\".format(np.min(len_text)))\n",
    "print(\"text의 최대 길이 : {}\".format(np.max(len_text)))\n",
    "print(\"text의 평균 길이 : {}\".format(np.mean(len_text)))\n",
    "below_threshold_len(max_len_text, df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565a9346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 19106\n",
      "삭제된 샘플수: 307\n"
     ]
    }
   ],
   "source": [
    "before = len(df)\n",
    "def is_within(text, max_len):\n",
    "    return len(text.split()) <= max_len\n",
    "_filter = df[\"text\"].apply(is_within, max_len=max_len_text)\n",
    "df = df[_filter]\n",
    "print(\"전체 샘플수 :\", (len(df)))\n",
    "print(f\"삭제된 샘플수: {before - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4061e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS 번호 : [8817]\n",
      "EOS 번호 : [8818]\n",
      "8819\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    df[\"text\"],\n",
    "    target_vocab_size=2**13,\n",
    ")\n",
    "SOS, EOS = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"SOS 번호 :\", [tokenizer.vocab_size])\n",
    "print(\"EOS 번호 :\", [tokenizer.vocab_size + 1])\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c826bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 19106\n"
     ]
    }
   ],
   "source": [
    "def tokenize(texts):\n",
    "    tokenized = []\n",
    "    max_len = 0\n",
    "    for text in texts:\n",
    "        tokenized_txt = SOS + tokenizer.encode(text) + EOS\n",
    "        max_len = max(max_len, len(tokenized_txt))\n",
    "        tokenized.append(tokenized_txt)\n",
    "\n",
    "    # max_length 으로 모든 데이터셋을 패딩\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized, maxlen=max_len, padding=\"pre\"\n",
    "    ), max_len\n",
    "\n",
    "\n",
    "texts, MAX_LENGTH = tokenize(df[\"text\"])\n",
    "print(MAX_LENGTH, len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479ecbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,) 19106\n"
     ]
    }
   ],
   "source": [
    "print(texts[0].shape, len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87687bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': texts[:,:-1], # eos 토큰 제외\n",
    "    },\n",
    "    {\n",
    "        'outputs': texts[:, 1:] # sos 토큰 제외\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86fd5cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'inputs': <tf.Tensor: shape=(64, 19), dtype=int32, numpy=\n",
      "array([[   0,    0,    0, ..., 5396, 6776,  313],\n",
      "       [   0,    0,    0, ..., 4927, 3307,   72],\n",
      "       [   0,    0,    0, ...,   25, 8134, 1390],\n",
      "       ...,\n",
      "       [   0,    0,    0, ..., 6578,  211,  148],\n",
      "       [   0,    0,    0, ...,  388,  185,    1],\n",
      "       [   0,    0,    0, ..., 1395, 2760,  533]], dtype=int32)>}, {'outputs': <tf.Tensor: shape=(64, 19), dtype=int32, numpy=\n",
      "array([[   0,    0,    0, ..., 6776,  313, 8818],\n",
      "       [   0,    0,    0, ..., 3307,   72, 8818],\n",
      "       [   0,    0,    0, ..., 8134, 1390, 8818],\n",
      "       ...,\n",
      "       [   0,    0,    0, ...,  211,  148, 8818],\n",
      "       [   0,    0,    0, ...,  185,    1, 8818],\n",
      "       [   0,    0,    0, ..., 2760,  533, 8818]], dtype=int32)>})\n"
     ]
    }
   ],
   "source": [
    "for data in dataset.take(1):\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ea6aa",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61400920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    query: (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    key: (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    value: (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9  # FYI, 0은 softmax에서 양수값을 가진다.\n",
    "\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a879d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64 (512 // 8)\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(\n",
    "            inputs, perm=[0, 2, 1, 3]\n",
    "        )  # (batch, heads, max 문장 토큰 갯수, 64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68316c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    # x (batch_size, max 문장 토큰 수)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "# 가릴곳: 1, 참조할곳: 0\n",
    "def create_look_ahead_mask(x):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e38a927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n",
       "array([[-0.04477648,  0.02605108],\n",
       "       [ 0.02280302,  0.02852701],\n",
       "       [-0.0255352 ,  0.04006446],\n",
       "       [ 0.03522268, -0.01556621],\n",
       "       [ 0.01954217,  0.02878943],\n",
       "       [ 0.04505612,  0.02069017],\n",
       "       [ 0.04850221,  0.02092388],\n",
       "       [ 0.03063026,  0.03347791],\n",
       "       [ 0.04661665, -0.01304621],\n",
       "       [ 0.00839097, -0.03884982],\n",
       "       [ 0.00209443, -0.0461431 ],\n",
       "       [-0.0206119 , -0.032855  ],\n",
       "       [-0.04517735,  0.03406909],\n",
       "       [ 0.03843451, -0.03344806],\n",
       "       [-0.04748671,  0.02191739],\n",
       "       [-0.04776806, -0.04471452],\n",
       "       [ 0.04883963, -0.02429321],\n",
       "       [-0.04370322,  0.00925354],\n",
       "       [ 0.03118126,  0.00491212],\n",
       "       [ 0.01271736, -0.04705909]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = tf.keras.layers.Embedding(input_dim=20, output_dim=2, name='test')\n",
    "pos = tf.range(start=0, limit=20, delta=1)\n",
    "emb(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b843fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras-nlp/blob/4aa0503073e21c86333dfc551e6d3443e1bdd017/keras_nlp/src/layers/modeling/position_embedding.py#L21\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_sequence_length,\n",
    "        d_model,\n",
    "        initializer,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=max_sequence_length,\n",
    "            output_dim=d_model,\n",
    "            name=self.name,\n",
    "            embeddings_initializer=initializer\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = tf.shape(inputs)[-2] #(...., seq_length, d_model)\n",
    "        pos = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        return inputs + self.embedding(pos)\n",
    "\n",
    "    \n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "\n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention1)\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 임베딩\n",
    "    embeddings = PositionalEmbedding(\n",
    "        initializer = tf.keras.initializers.RandomNormal(\n",
    "            mean=0.0, \n",
    "            stddev=0.02\n",
    "        ),        \n",
    "        name='positional_embedding',\n",
    "        max_sequence_length=seq_length,\n",
    "        d_model=d_model,\n",
    "    )(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5df26f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT1(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"transformer\",\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(inputs)\n",
    "\n",
    "    # 디코더\n",
    "    outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "    )(inputs=[inputs, look_ahead_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b32f1c",
   "metadata": {},
   "source": [
    "[GPT1 Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
    "\n",
    "#### Model specifications \n",
    "Our model largely follows the original transformer work [62]. We trained a\n",
    "12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12\n",
    "attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.\n",
    "We used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate\n",
    "was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule.\n",
    "We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\n",
    "Since layernorm [2] is used extensively throughout the model, a simple weight initialization of\n",
    "N(0, 0.02) was sufficient. We used a bytepair encoding (BPE) vocabulary with 40,000 merges [53]\n",
    "and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also\n",
    "employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or\n",
    "gain weights. For the activation function, we used the Gaussian Error Linear Unit (GELU) [18]. We\n",
    "used learned position embeddings instead of the sinusoidal version proposed in the original work.\n",
    "We use the ftfy library2\n",
    "to clean the raw text in BooksCorpus, standardize some punctuation and\n",
    "whitespace, and use the spaCy tokenizer.3\n",
    "\n",
    "### Fine-tuning details\n",
    "Unless specified, we reuse the hyperparameter settings from unsupervised\n",
    "pre-training. We add dropout to the classifier with a rate of 0.1. For most tasks, we use a learning rate\n",
    "of 6.25e-5 and a batchsize of 32. Our model finetunes quickly and 3 epochs of training was sufficient\n",
    "for most cases. We use a linear learning rate decay schedule with warmup over 0.2% of training. λ\n",
    "was set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111ec8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 768)    91842816    inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8819)   6781811     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 98,624,627\n",
      "Trainable params: 98,624,627\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# 하이퍼파라미터\n",
    "# https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "# Model speci\n",
    "NUM_LAYERS = 12  # GPT의 경우 인코더 층의 개수로 설정됨\n",
    "D_MODEL = 768  # GPT 모델에서 사용된 모델 크기\n",
    "NUM_HEADS = 12  # GPT에서 사용된 멀티 헤드 어텐션의 헤드 수\n",
    "UNITS = 3072  # GPT에서 사용된 피드 포워드 신경망의 은닉층 크기\n",
    "DROPOUT = 0.1  # GPT에서 사용된 드롭아웃 비율\n",
    "EPOCHS=100\n",
    "\n",
    "model = GPT1(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    seq_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b8a92af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "299/299 [==============================] - 90s 259ms/step - loss: 2.6682 - accuracy: 0.0941\n",
      "Epoch 2/100\n",
      "299/299 [==============================] - 76s 254ms/step - loss: 2.3816 - accuracy: 0.1052\n",
      "Epoch 3/100\n",
      "299/299 [==============================] - 75s 252ms/step - loss: 2.4929 - accuracy: 0.0733\n",
      "Epoch 4/100\n",
      "299/299 [==============================] - 75s 252ms/step - loss: 2.5585 - accuracy: 0.0567\n",
      "Epoch 5/100\n",
      "299/299 [==============================] - 75s 251ms/step - loss: 2.4917 - accuracy: 0.0728\n",
      "Epoch 6/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.3615 - accuracy: 0.1025\n",
      "Epoch 7/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.3155 - accuracy: 0.1046\n",
      "Epoch 8/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2938 - accuracy: 0.1061\n",
      "Epoch 9/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2884 - accuracy: 0.1061\n",
      "Epoch 10/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.3332 - accuracy: 0.1002\n",
      "Epoch 11/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2853 - accuracy: 0.1061\n",
      "Epoch 12/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.3023 - accuracy: 0.1039\n",
      "Epoch 13/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2962 - accuracy: 0.1050\n",
      "Epoch 14/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2941 - accuracy: 0.1041\n",
      "Epoch 15/100\n",
      "299/299 [==============================] - 75s 249ms/step - loss: 2.2869 - accuracy: 0.1053\n",
      "Epoch 16/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2702 - accuracy: 0.1064\n",
      "Epoch 17/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2939 - accuracy: 0.1050\n",
      "Epoch 18/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2703 - accuracy: 0.1060\n",
      "Epoch 19/100\n",
      "299/299 [==============================] - 75s 249ms/step - loss: 2.2654 - accuracy: 0.1063\n",
      "Epoch 20/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2785 - accuracy: 0.1039\n",
      "Epoch 21/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2647 - accuracy: 0.1061\n",
      "Epoch 22/100\n",
      "299/299 [==============================] - 75s 249ms/step - loss: 2.2560 - accuracy: 0.1066\n",
      "Epoch 23/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2587 - accuracy: 0.1065\n",
      "Epoch 24/100\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 2.2519 - accuracy: 0.1067\n",
      "Epoch 25/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2482 - accuracy: 0.1068\n",
      "Epoch 26/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2528 - accuracy: 0.1065\n",
      "Epoch 27/100\n",
      "299/299 [==============================] - 75s 249ms/step - loss: 2.2712 - accuracy: 0.1043\n",
      "Epoch 28/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.4038 - accuracy: 0.0830\n",
      "Epoch 29/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.3282 - accuracy: 0.0925\n",
      "Epoch 30/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2978 - accuracy: 0.0986\n",
      "Epoch 31/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2972 - accuracy: 0.0977\n",
      "Epoch 32/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2855 - accuracy: 0.1000\n",
      "Epoch 33/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2788 - accuracy: 0.1008\n",
      "Epoch 34/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2734 - accuracy: 0.1018\n",
      "Epoch 35/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2677 - accuracy: 0.1028\n",
      "Epoch 36/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2604 - accuracy: 0.1041\n",
      "Epoch 37/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2558 - accuracy: 0.1048\n",
      "Epoch 38/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2531 - accuracy: 0.1050\n",
      "Epoch 39/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2533 - accuracy: 0.1050\n",
      "Epoch 40/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2511 - accuracy: 0.1051\n",
      "Epoch 41/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2489 - accuracy: 0.1053\n",
      "Epoch 42/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2465 - accuracy: 0.1055\n",
      "Epoch 43/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2467 - accuracy: 0.1055\n",
      "Epoch 44/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2465 - accuracy: 0.1057\n",
      "Epoch 45/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2443 - accuracy: 0.1056\n",
      "Epoch 46/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2439 - accuracy: 0.1058\n",
      "Epoch 47/100\n",
      "299/299 [==============================] - 74s 249ms/step - loss: 2.2444 - accuracy: 0.1057\n",
      "Epoch 48/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2429 - accuracy: 0.1058\n",
      "Epoch 49/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2442 - accuracy: 0.1057\n",
      "Epoch 50/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2403 - accuracy: 0.1059\n",
      "Epoch 51/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2409 - accuracy: 0.1058\n",
      "Epoch 52/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2408 - accuracy: 0.1058\n",
      "Epoch 53/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2405 - accuracy: 0.1059\n",
      "Epoch 54/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2387 - accuracy: 0.1060\n",
      "Epoch 55/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2369 - accuracy: 0.1061\n",
      "Epoch 56/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2365 - accuracy: 0.1061\n",
      "Epoch 57/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2363 - accuracy: 0.1061\n",
      "Epoch 58/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2357 - accuracy: 0.1062\n",
      "Epoch 59/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2347 - accuracy: 0.1062\n",
      "Epoch 60/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2349 - accuracy: 0.1063\n",
      "Epoch 61/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2335 - accuracy: 0.1062\n",
      "Epoch 62/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2345 - accuracy: 0.1062\n",
      "Epoch 63/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2344 - accuracy: 0.1061\n",
      "Epoch 64/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2318 - accuracy: 0.1063\n",
      "Epoch 65/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2335 - accuracy: 0.1062\n",
      "Epoch 66/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2334 - accuracy: 0.1062\n",
      "Epoch 67/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2308 - accuracy: 0.1064\n",
      "Epoch 68/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2301 - accuracy: 0.1064\n",
      "Epoch 69/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2311 - accuracy: 0.1064\n",
      "Epoch 70/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2304 - accuracy: 0.1064\n",
      "Epoch 71/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2301 - accuracy: 0.1065\n",
      "Epoch 72/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2301 - accuracy: 0.1064\n",
      "Epoch 73/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2296 - accuracy: 0.1065\n",
      "Epoch 74/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2295 - accuracy: 0.1064\n",
      "Epoch 75/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2297 - accuracy: 0.1064\n",
      "Epoch 76/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2291 - accuracy: 0.1064\n",
      "Epoch 77/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2290 - accuracy: 0.1065\n",
      "Epoch 78/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2271 - accuracy: 0.1066\n",
      "Epoch 79/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2292 - accuracy: 0.1065\n",
      "Epoch 80/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2294 - accuracy: 0.1064\n",
      "Epoch 81/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2301 - accuracy: 0.1065\n",
      "Epoch 82/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2285 - accuracy: 0.1064\n",
      "Epoch 83/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2283 - accuracy: 0.1065\n",
      "Epoch 84/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2277 - accuracy: 0.1066\n",
      "Epoch 85/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2282 - accuracy: 0.1064\n",
      "Epoch 86/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2274 - accuracy: 0.1067\n",
      "Epoch 87/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2270 - accuracy: 0.1065\n",
      "Epoch 88/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2266 - accuracy: 0.1066\n",
      "Epoch 89/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2278 - accuracy: 0.1066\n",
      "Epoch 90/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2272 - accuracy: 0.1066\n",
      "Epoch 91/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2264 - accuracy: 0.1066\n",
      "Epoch 92/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2285 - accuracy: 0.1065\n",
      "Epoch 93/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2275 - accuracy: 0.1065\n",
      "Epoch 94/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2269 - accuracy: 0.1066\n",
      "Epoch 95/100\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 2.2264 - accuracy: 0.1066\n",
      "Epoch 96/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2264 - accuracy: 0.1067\n",
      "Epoch 97/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2258 - accuracy: 0.1066\n",
      "Epoch 98/100\n",
      "299/299 [==============================] - 74s 246ms/step - loss: 2.2269 - accuracy: 0.1065\n",
      "Epoch 99/100\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 2.2257 - accuracy: 0.1067\n",
      "Epoch 100/100\n",
      "299/299 [==============================] - 74s 246ms/step - loss: 2.2259 - accuracy: 0.1066\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss_function, \n",
    "    metrics=[accuracy],\n",
    ")\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d6429e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(predictions, top_k):\n",
    "    top_k_indices = tf.argsort(predictions, axis=-1, direction='DESCENDING')[:, :, :top_k]\n",
    "    random_index = tf.random.uniform(shape=[], minval=0, maxval=top_k, dtype=tf.int32)\n",
    "    predicted_id = tf.gather(top_k_indices, random_index, axis=-1) # predicted_id = top_k_indices[0, 0, random_index]\n",
    "    predicted_id = tf.cast(predicted_id, tf.int32)\n",
    "    return predicted_id\n",
    "\n",
    "def decoder_inference(sentence, top_k=10):\n",
    "    sentence = preprocess(sentence)\n",
    "    inputs = tf.expand_dims(SOS + tokenizer.encode(sentence) + EOS, axis=0)\n",
    "    outputs = tf.expand_dims(SOS, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs[:,i:], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = top_k_sample(predictions, top_k)\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, EOS[0]):  # end_token_id\n",
    "            break\n",
    "\n",
    "        inputs = tf.concat([inputs, predicted_id], axis=-1)\n",
    "        outputs = tf.concat([outputs, predicted_id], axis=-1)\n",
    "    return tf.squeeze(outputs, axis=0)\n",
    "\n",
    "\n",
    "def sentence_generation(sentence):\n",
    "    prediction = decoder_inference(sentence)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    print(f\"🧑 : {sentence}\")\n",
    "    print(f\"🤖 : {predicted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4e597ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 : 무슨마음일까\n",
      "🤖 : 썸 내가 너무 좋은 좋은 너무 남자친구가 잘 오늘 좋은 좋은 좋은 남자친구가 나 잘 오늘 너무 좋아하는 남자친구가 나 \n",
      "🧑 : 손길이 가서 더 애정이 생길 것 같아요 .\n",
      "🤖 : 오늘 오늘 나 너무 남자친구가 남자친구가 내가 잘 이제 남자친구가 오늘 나 너무 잘 이제 나 좋아하는 썸 이제 잘 \n",
      "🧑 : 배고파\n",
      "🤖 : 나 내가 내가 좋은 이제 이제 내가 내가 내가 너무 오늘 좋은 좋은 남자친구가 내가 좋은 나 내가 너무 나 \n",
      "🧑 : 썸녀가 그냥 오빠 동생으로 지내자 했음 .\n",
      "🤖 : 좋아하는 이제 너무 오늘 내가 오늘 이제 오늘 좋은 남자친구가 너무 좋은 나 내가 너무 너무 좋은 좋은 남자친구가 나 \n",
      "🧑 : 힘든 것 좀 끝났으면\n",
      "🤖 : 이제 나 내가 좋아하는 남자친구가 남자친구가 썸 좋아하는 잘 남자친구가 오늘 남자친구가 좋은 좋아하는 좋아하는 너무 오늘 오늘 잘 이제 \n"
     ]
    }
   ],
   "source": [
    "for _, q in df.sample(5).iterrows():\n",
    "    question = q['text']\n",
    "    sentence_generation(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aa5c4b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 : 느그서장 남천동 살제?\n",
      "🤖 : 이제 좋아하는 내가 너무 썸 나 남자친구가 남자친구가 썸 좋은 너무 오늘 오늘 잘 이제 나 오늘 나 좋은 이제 \n"
     ]
    }
   ],
   "source": [
    "sentence_generation(\"느그서장 남천동 살제?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5837bdb",
   "metadata": {},
   "source": [
    "## retrospect\n",
    "\n",
    "```py\n",
    "predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "```\n",
    "\n",
    "- 위 방식을 사용하니 '좋아하는'이라는 토큰만 나오는 문제점이 발생했다. 이 때문에 top 10개를 선별해서, random하게 return해주었다.\n",
    "\n",
    "\n",
    ">Q. inputs에 prediction을 추가해서 for loop를 돌면서 예측해야 할지, 아니면 inputs[:,i]를 통해서, 문장이 predict될 때마다 앞쪽 token을 지워줘야 하는지 모르겠다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc292a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
