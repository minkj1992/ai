{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e387428",
   "metadata": {},
   "source": [
    "# 1. GPT ëª¨ë¸\n",
    "\n",
    "ê¸°ì¡´ì˜ encoder-decoder êµ¬ì¡°ì¸ Transformer ëª¨ë¸ì„ ë³€í˜•í•˜ì—¬ GPT ëª¨ë¸ êµ¬ì¡°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì€ ë³€ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë•Œ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ëŠ” ì‚¬ë¼ì ¸ì•¼í•  ì»´í¬ë„ŒíŠ¸ì´ë©°, ì´ˆë¡ìƒ‰ ë°•ìŠ¤ëŠ” ë³€ê²½ë˜ì–´ì•¼ í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "![](https://github.com/minkj1992/ai/blob/main/static/Untitled-2024-06-21-1948.png?raw=true)\n",
    "\n",
    "openaiì—ì„œ ë°œí‘œí•œ [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) ë…¼ë¬¸ì— ë”°ë¥´ë©´, decoder ëª¨ë¸ì„ í† ëŒ€ë¡œ generalizedí•œ pre-trained ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ transformerì˜ ì‚¬ë¼ì ¸ì•¼í•  ë¶€ë¶„ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### ì‚¬ë¼ì ¸ì•¼í•  ë¶€ë¶„\n",
    "> ë¹¨ê°„ ë„¤ëª¨\n",
    "1. `Encoder`\n",
    "2. `encoder-decoder attention layer`\n",
    "\n",
    "\n",
    "ìœ„ ë…¼ë¬¸ì— ë”°ë¥´ë©´ ë³€ê²½ë˜ì–´ì•¼í•  ë¶€ë¶„ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "#### ë³€ê²½ë˜ì–´ì•¼ í•  ë¶€ë¶„\n",
    "> ì´ˆë¡ ë„¤ëª¨\n",
    "\n",
    "1. Positional encoding -> Positional embedding\n",
    "2. Target input -> `Task-specific input transformations`\n",
    "\n",
    "í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ë˜í•œ ê¸°ì¡´ì˜ sin,così„ í™œìš©í•œ positional ê³„ì‚° ë°©ì‹ì„ positional embedding (max_sentence_len, d_model)ì„ ìƒì„±í•˜ì—¬ í•™ìŠµë˜ë„ë¡ í•©ë‹ˆë‹¤. \n",
    "\n",
    "Target input ë˜í•œ ë‹¬ë¼ì ¸ì•¼ í•˜ëŠ”ë°, ì´ë²ˆ ì‹œê°„ì—ëŠ” pretrain + fine tunning ì´ 2ë²ˆ í•™ìŠµí–ˆë˜ ë…¼ë¬¸ê³¼ ë‹¬ë¦¬ 1ë²ˆë§Œ í•™ìŠµí•  ì˜ˆì •ì´ê¸° ë•Œë¬¸ì—, inputì„ ë³€í˜•í•´ì„œ ë„£ì–´ì£¼ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤. \n",
    "\n",
    "í˜„ì¬ ë°ì´í„° ì…‹ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì¸, df['Q']ì™€ ì±—ë´‡ì˜ ë‹µë³€ì¸ df['A']ê°€ ì¡´ì¬í•˜ë©°, ì´ ë‘ ë°ì´í„° ëª¨ë‘ ì±—ë´‡ì— í•„ìš”í•˜ë©° ë¶€ì¡±í•œ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ì—¬, í•©ì³ì„œ í•™ìŠµí•´ì£¼ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1dd2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6d2a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11823, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10819</th>\n",
       "      <td>ì˜¤ë«ë™ì•ˆ ì¸ë§Œ íƒ€. ë­ê°€ ë¬¸ì œì„?</td>\n",
       "      <td>ê³ ë°±ì„ ì•ˆ í•œ ê²Œ ë¬¸ì œì¼ ê±° ê°™ì•„ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>ì¹œêµ¬ë‘ ì‹¸ì› ì–´</td>\n",
       "      <td>ì‹¸ìš°ë©´ì„œ ì • ë“¤ ê±°ì˜ˆìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ê¹Œ</td>\n",
       "      <td>íƒœì—°í•˜ê²Œ ì‹œì‘í•´ë³´ì„¸ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>ê¸°ë¶„ì´ ì´ìƒí•´</td>\n",
       "      <td>ë¬´ìŠ¨ ì´ìœ ì¸ì§€ ìƒê°í•´ë³´ì„¸ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>ê³ ê¸° ë¨¹ê³  ì‹¶ì–´</td>\n",
       "      <td>ì €ê¸°ì••ì—ëŠ” ê³ ê¸°ì•ì´ì£ .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>ì–´ì´ì—†ì–´</td>\n",
       "      <td>ê·¸ëƒ¥ ìŠì–´ë²„ë¦¬ì„¸ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>ë°”ë‹¤ ê°€ìê³  í•˜ë©´ ê°ˆê¹Œ?</td>\n",
       "      <td>ê°™ì´ ê°€ìê³  ë§í•´ë³´ì„¸ìš”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7213</th>\n",
       "      <td>ì˜¨ì „íˆ ë‚´ í¸ì´ê¸¸ ë°”ë¼ëŠ” ëˆ„êµ°ê°€ê°€ ì˜†ì— ìˆì—ˆìœ¼ë©´ ì¢‹ê² ëŠ”</td>\n",
       "      <td>ì œê°€ ë‹¹ì‹  ê³ì— ìˆì–´ë“œë¦´ê²Œìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>ë‚´ì—¬ì˜ ê±´ë“œë¦°ë†ˆë“¤</td>\n",
       "      <td>ë²Œ ë°›ì„ ê±°ì˜ˆìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9591</th>\n",
       "      <td>ë‹¤ì‹œ ì›ƒê²Œ í•´ì¤€ ì‚¬ëŒ</td>\n",
       "      <td>ê·¸ ì‚¬ëŒì´ ë‹¹ì‹ ì˜ í™ˆë‹¥í„°êµ°ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>ëˆˆì„ ê°ì•„ë„ ë³´ê³  ì‹¶ì–´</td>\n",
       "      <td>ìƒì‚¬ë³‘ ê±¸ë¦¬ê¸°ì „ì— ì°¾ì•„ê°€ë³´ì„¸ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>ì´ë³„ í›„ í•œë‹¬ì§¸</td>\n",
       "      <td>ì•„ì§ ë§ì´ í˜ë“œì‹œê² êµ°ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>ë²„ë¦´ ê±´ ë²„ë ¤ì•¼ ê² ì§€</td>\n",
       "      <td>ì˜ ë²„ë¦¬ëŠ” ê²ƒë„ ì¤‘ìš”í•´ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10820</th>\n",
       "      <td>ì˜¤ë«ë™ì•ˆ ì¸ë§Œ íƒ€ê³  ì†”ë¡œì¸ê±´ ì™œê·¸ë˜?</td>\n",
       "      <td>ì¸ì´ ì•„ë‹ˆì—ˆì„ì§€ë„ ëª°ë¼ìš”.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>ìŠ¤ì¼€ì¼ë§ í•˜ê¸° ê·€ì°®ì•„</td>\n",
       "      <td>ì£¼ê¸°ì ìœ¼ë¡œ í•´ì£¼ë©´ ì¢‹ëŒ€ìš”.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Q                     A  label\n",
       "10819              ì˜¤ë«ë™ì•ˆ ì¸ë§Œ íƒ€. ë­ê°€ ë¬¸ì œì„?  ê³ ë°±ì„ ì•ˆ í•œ ê²Œ ë¬¸ì œì¼ ê±° ê°™ì•„ìš”.      2\n",
       "4659                          ì¹œêµ¬ë‘ ì‹¸ì› ì–´         ì‹¸ìš°ë©´ì„œ ì • ë“¤ ê±°ì˜ˆìš”.      0\n",
       "6939                      ì–´ë–»ê²Œ ì‚´ì•„ê°€ì•¼ í• ê¹Œ          íƒœì—°í•˜ê²Œ ì‹œì‘í•´ë³´ì„¸ìš”.      1\n",
       "376                           ê¸°ë¶„ì´ ì´ìƒí•´       ë¬´ìŠ¨ ì´ìœ ì¸ì§€ ìƒê°í•´ë³´ì„¸ìš”.      0\n",
       "181                          ê³ ê¸° ë¨¹ê³  ì‹¶ì–´          ì €ê¸°ì••ì—ëŠ” ê³ ê¸°ì•ì´ì£ .      0\n",
       "3058                             ì–´ì´ì—†ì–´            ê·¸ëƒ¥ ìŠì–´ë²„ë¦¬ì„¸ìš”.      0\n",
       "1891                    ë°”ë‹¤ ê°€ìê³  í•˜ë©´ ê°ˆê¹Œ?          ê°™ì´ ê°€ìê³  ë§í•´ë³´ì„¸ìš”      0\n",
       "7213   ì˜¨ì „íˆ ë‚´ í¸ì´ê¸¸ ë°”ë¼ëŠ” ëˆ„êµ°ê°€ê°€ ì˜†ì— ìˆì—ˆìœ¼ë©´ ì¢‹ê² ëŠ”      ì œê°€ ë‹¹ì‹  ê³ì— ìˆì–´ë“œë¦´ê²Œìš”.      1\n",
       "5893                        ë‚´ì—¬ì˜ ê±´ë“œë¦°ë†ˆë“¤             ë²Œ ë°›ì„ ê±°ì˜ˆìš”.      1\n",
       "9591                      ë‹¤ì‹œ ì›ƒê²Œ í•´ì¤€ ì‚¬ëŒ      ê·¸ ì‚¬ëŒì´ ë‹¹ì‹ ì˜ í™ˆë‹¥í„°êµ°ìš”.      2\n",
       "9580                     ëˆˆì„ ê°ì•„ë„ ë³´ê³  ì‹¶ì–´     ìƒì‚¬ë³‘ ê±¸ë¦¬ê¸°ì „ì— ì°¾ì•„ê°€ë³´ì„¸ìš”.      2\n",
       "7473                         ì´ë³„ í›„ í•œë‹¬ì§¸         ì•„ì§ ë§ì´ í˜ë“œì‹œê² êµ°ìš”.      1\n",
       "2028                      ë²„ë¦´ ê±´ ë²„ë ¤ì•¼ ê² ì§€        ì˜ ë²„ë¦¬ëŠ” ê²ƒë„ ì¤‘ìš”í•´ìš”.      0\n",
       "10820            ì˜¤ë«ë™ì•ˆ ì¸ë§Œ íƒ€ê³  ì†”ë¡œì¸ê±´ ì™œê·¸ë˜?        ì¸ì´ ì•„ë‹ˆì—ˆì„ì§€ë„ ëª°ë¼ìš”.      2\n",
       "2655                      ìŠ¤ì¼€ì¼ë§ í•˜ê¸° ê·€ì°®ì•„        ì£¼ê¸°ì ìœ¼ë¡œ í•´ì£¼ë©´ ì¢‹ëŒ€ìš”.      0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/ChatbotData .csv' \n",
    "origin = pd.read_csv(path) \n",
    "\n",
    "print(origin.shape)\n",
    "origin.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15b49c",
   "metadata": {},
   "source": [
    "# 1. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd314645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23646, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>ê°ì •ì¡°ì ˆì´ ì•ˆë˜ì„œ ë¬¸ì ë³´ëƒˆë„¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10735</th>\n",
       "      <td>ì—°ì• ë³´ë‹¤ ì¸ì´ ì¢‹ì•„.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13404</th>\n",
       "      <td>ì„œë¡œ ë§ˆìŒë§Œ ë§ìœ¼ë©´ ê°€ëŠ¥í•´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12714</th>\n",
       "      <td>ê³µë¶€í•œ ë§Œí¼ ë‚˜ì˜¬ ê±°ì˜ˆìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>ìŠ¤í‚¤ ê°•ìŠµ ë°›ì•„ì•¼ ë ê¹Œ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12926</th>\n",
       "      <td>ê¿ˆê°™ì€ ì´ì•¼ê¸°ë„¤ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>ì‹ìš•ì´ ì—†ì–´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>ë„ˆí•œí…Œ ì“°ëŠ” í¸ì§€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>í•˜ë£¨í•˜ë£¨ê°€ ì§€ì˜¥ê°™ì•„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>ìŠ¬í”ˆ ì˜ˆê°ëŒ€ë¡œ ë˜ì–´ê°€ëŠ” í˜„ì‹¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>ë§ˆìŒì´ ìš¸ì í•´</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23074</th>\n",
       "      <td>ì„±ê²©ë„ ê³„ì† ë°”ë€Œë‹ˆ ê±±ì • ë§ì•„ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>ë‘ë‹¬ ë°˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>ë„ˆ ë˜ ë­í•  ì¤„ ì•Œì•„?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>ì˜¤ëŠ˜ ë˜ ëŠ¦ì ì¤ì–´</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text\n",
       "5448     ê°ì •ì¡°ì ˆì´ ì•ˆë˜ì„œ ë¬¸ì ë³´ëƒˆë„¤\n",
       "10735         ì—°ì• ë³´ë‹¤ ì¸ì´ ì¢‹ì•„.\n",
       "13404    ì„œë¡œ ë§ˆìŒë§Œ ë§ìœ¼ë©´ ê°€ëŠ¥í•´ìš”.\n",
       "12714      ê³µë¶€í•œ ë§Œí¼ ë‚˜ì˜¬ ê±°ì˜ˆìš”.\n",
       "2658        ìŠ¤í‚¤ ê°•ìŠµ ë°›ì•„ì•¼ ë ê¹Œ?\n",
       "12926          ê¿ˆê°™ì€ ì´ì•¼ê¸°ë„¤ìš”.\n",
       "2763               ì‹ìš•ì´ ì—†ì–´\n",
       "5973            ë„ˆí•œí…Œ ì“°ëŠ” í¸ì§€\n",
       "8412           í•˜ë£¨í•˜ë£¨ê°€ ì§€ì˜¥ê°™ì•„\n",
       "6758      ìŠ¬í”ˆ ì˜ˆê°ëŒ€ë¡œ ë˜ì–´ê°€ëŠ” í˜„ì‹¤\n",
       "1481              ë§ˆìŒì´ ìš¸ì í•´\n",
       "23074  ì„±ê²©ë„ ê³„ì† ë°”ë€Œë‹ˆ ê±±ì • ë§ì•„ìš”.\n",
       "6109                 ë‘ë‹¬ ë°˜\n",
       "929          ë„ˆ ë˜ ë­í•  ì¤„ ì•Œì•„?\n",
       "3286            ì˜¤ëŠ˜ ë˜ ëŠ¦ì ì¤ì–´"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = origin.copy()\n",
    "\n",
    "df_q = pd.DataFrame({\n",
    "    'text': tmp['Q']\n",
    "})\n",
    "df_a = pd.DataFrame({\n",
    "    'text': tmp['A']\n",
    "})\n",
    "\n",
    "df = pd.concat([df_q, df_a], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5ecac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23646, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e43e105f3a42e78e7b163794f44448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19436 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19436, 1)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ê°€-í£a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def postprocess(df):\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.drop_duplicates(subset = ['text'], inplace = True)\n",
    "df['text'] = df['text'].progress_apply(preprocess)\n",
    "postprocess(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88629e27",
   "metadata": {},
   "source": [
    "## 2. Data prepare\n",
    "\n",
    "- ì§ˆë¬¸,ë‹µë³€ ê°ì ì ì ˆí•œ maximum í¬ê¸°ë§Œ ë‚¨ê¸°ê³  ì§€ìš´ë‹¤.\n",
    "- `[SOS] ì§ˆë¬¸ [DELIM] ë‹µë³€ [EOS]` í•©ì¹œë‹¤ -> teacher forcing í•™ìŠµ\n",
    "- ì¶”ë¡ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ -> ì±—ë´‡ ë‹µë³€ì—ì„œ [DELIM] ~ [EOS]ê¹Œì§€ë§Œ return.\n",
    "\n",
    "\n",
    "## (WIP) ì¶”í›„ ë„ì… ë°©ë²•\n",
    "1. Genralized Pretrain\n",
    "2. Fine tunning\n",
    "\n",
    "ë‘ê°€ì§€ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ 2ê°€ì§€ì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "1ë²ˆì„ ìœ„í•´ì„œëŠ” í•˜ë‚˜ì˜ rowì— ì†í•œ df['Q']ì™€ df['A']ë¥¼ 2ê°œì˜ rowë¡œ ë‚˜ëˆ ì„œ df['text']ë¡œ ë§Œë“¤ì–´ 0...i -> i+1ì˜ˆì¸¡ teacher forcingì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„ 2ë²ˆì€ `[SOS] Q [DELIM] A_set [EOS]`í˜•íƒœì˜ ë¬¸ì¥ì„ A_setë§Œí¼ ë§Œë“¤ì–´, ì •ë‹µì¸ Aë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” softmaxë¥¼ ìƒì„±, ì´ë¥¼ ìœ„í•´ì„œëŠ” ì •ë‹µì´ ì•„ë‹Œ negative samplingì´ í•„ìš”í•´ì„œ, ë‹¤ìŒ ê¸°íšŒì— ì‹œë„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f912ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textì˜ ìµœì†Œ ê¸¸ì´ : 1\n",
      "textì˜ ìµœëŒ€ ê¸¸ì´ : 24\n",
      "textì˜ í‰ê·  ê¸¸ì´ : 4.315651368594361\n",
      "ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ 9 ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: 0.9842045688413253\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if len(s.split()) <= max_len:\n",
    "            cnt = cnt + 1\n",
    "    print(\n",
    "        \"ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s\"\n",
    "        % (max_len, (cnt / len(nested_list)))\n",
    "    )\n",
    "\n",
    "    \n",
    "len_text = [len(s.split()) for s in df['text']]\n",
    "max_len_text = 9\n",
    "print(\"textì˜ ìµœì†Œ ê¸¸ì´ : {}\".format(np.min(len_text)))\n",
    "print(\"textì˜ ìµœëŒ€ ê¸¸ì´ : {}\".format(np.max(len_text)))\n",
    "print(\"textì˜ í‰ê·  ê¸¸ì´ : {}\".format(np.mean(len_text)))\n",
    "below_threshold_len(max_len_text, df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b942357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒ˜í”Œìˆ˜ : 19129\n",
      "ì‚­ì œëœ ìƒ˜í”Œìˆ˜: 307\n"
     ]
    }
   ],
   "source": [
    "before = len(df)\n",
    "def is_within(text, max_len):\n",
    "    return len(text.split()) <= max_len\n",
    "_filter = df[\"text\"].apply(is_within, max_len=max_len_text)\n",
    "df = df[_filter]\n",
    "print(\"ì „ì²´ ìƒ˜í”Œìˆ˜ :\", (len(df)))\n",
    "print(f\"ì‚­ì œëœ ìƒ˜í”Œìˆ˜: {before - len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c5f562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS ë²ˆí˜¸ : [8817]\n",
      "EOS ë²ˆí˜¸ : [8818]\n",
      "8819\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    df[\"text\"],\n",
    "    target_vocab_size=2**13,\n",
    ")\n",
    "SOS, EOS = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"SOS ë²ˆí˜¸ :\", [tokenizer.vocab_size])\n",
    "print(\"EOS ë²ˆí˜¸ :\", [tokenizer.vocab_size + 1])\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "745ae419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 19129\n"
     ]
    }
   ],
   "source": [
    "def tokenize(texts):\n",
    "    tokenized = []\n",
    "    max_len = 0\n",
    "    for text in texts:\n",
    "        tokenized_txt = SOS + tokenizer.encode(text) + EOS\n",
    "        max_len = max(max_len, len(tokenized_txt))\n",
    "        tokenized.append(tokenized_txt)\n",
    "\n",
    "    # max_length ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized, maxlen=max_len, padding=\"post\"\n",
    "    ), max_len\n",
    "\n",
    "\n",
    "texts, MAX_LENGTH = tokenize(df[\"text\"])\n",
    "print(MAX_LENGTH, len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1fc23bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19129"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].shape\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1327c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': texts[:,:-1],\n",
    "    },\n",
    "    {\n",
    "        'outputs': texts[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cc439",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36c828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    query: (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "    key: (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "    value: (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "    padding_mask : (batch_size, 1, 1, keyì˜ ë¬¸ì¥ ê¸¸ì´)\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9  # FYI, 0ì€ softmaxì—ì„œ ì–‘ìˆ˜ê°’ì„ ê°€ì§„ë‹¤.\n",
    "\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # output : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_modelì„ num_headsë¡œ ë‚˜ëˆˆ ê°’.\n",
    "        # ë…¼ë¬¸ ê¸°ì¤€ : 64 (512 // 8)\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì •ì˜\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # num_heads ê°œìˆ˜ë§Œí¼ q, k, vë¥¼ splití•˜ëŠ” í•¨ìˆ˜\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(\n",
    "            inputs, perm=[0, 2, 1, 3]\n",
    "        )  # (batch, heads, max ë¬¸ì¥ í† í° ê°¯ìˆ˜, 64)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 1. WQ, WK, WVì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸°\n",
    "        # q : (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n",
    "        # k : (batch_size, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n",
    "        # v : (batch_size, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n",
    "        # ì°¸ê³ ) ì¸ì½”ë”(k, v)-ë””ì½”ë”(q) ì–´í…ì…˜ì—ì„œëŠ” query ê¸¸ì´ì™€ key, valueì˜ ê¸¸ì´ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 2. í—¤ë“œ ë‚˜ëˆ„ê¸°\n",
    "        # q : (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, keyì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, valueì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 3. ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜. ì•ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ ì‚¬ìš©.\n",
    "        # (batch_size, num_heads, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 4. í—¤ë“œ ì—°ê²°(concatenate)í•˜ê¸°\n",
    "        # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 5. WOì— í•´ë‹¹í•˜ëŠ” ë°€ì§‘ì¸µ ì§€ë‚˜ê¸°\n",
    "        # (batch_size, queryì˜ ë¬¸ì¥ ê¸¸ì´, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b67dd38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    # x (batch_size, max ë¬¸ì¥ í† í° ìˆ˜)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "# ê°€ë¦´ê³³: 1, ì°¸ì¡°í• ê³³: 0\n",
    "def create_look_ahead_mask(x):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/952b8f05f42aaf27b083d0cd53946a2a2e9a4c69/official/nlp/modeling/layers/position_embedding.py#L27\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "        initializer=\"he_normal\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.initializer = tf.keras.initializers.get(initializer)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"initializer\": tf.keras.initializers.serialize(self.initializer),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        feature_size = inputs_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            name=\"embeddings\",\n",
    "            shape=[self.sequence_length, feature_size],\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, start_index=0):\n",
    "        shape = tf.shape(inputs)\n",
    "        feature_length = shape[-1]\n",
    "        sequence_length = shape[-2]\n",
    "        position_embeddings = tf.convert_to_tensor(self.position_embeddings)\n",
    "        position_embeddings = tf.slice(\n",
    "            position_embeddings,\n",
    "            (start_index, 0),\n",
    "            (sequence_length, feature_length),\n",
    "        )\n",
    "        return tf.broadcast_to(position_embeddings, shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape    \n",
    "    \n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "\n",
    "    # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention1)\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # í¬ì§€ì…”ë„ ì„ë² ë”©\n",
    "    embeddings = PositionalEmbedding(seq_length)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a36f4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, seq_length, name=\"transformer\",\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(inputs)\n",
    "\n",
    "    # ë””ì½”ë”\n",
    "    outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        seq_length=seq_length,\n",
    "    )(inputs=[inputs, look_ahead_mask])\n",
    "\n",
    "    # ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[\n",
    "            inputs,\n",
    "        ],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c5c8dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    12936704    inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8819)   4524147     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,460,851\n",
      "Trainable params: 17,460,851\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 12  # GPTì˜ ê²½ìš° ì¸ì½”ë” ì¸µì˜ ê°œìˆ˜ë¡œ ì„¤ì •ë¨\n",
    "D_MODEL = 768  # GPT ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ëª¨ë¸ í¬ê¸°\n",
    "NUM_HEADS = 12  # GPTì—ì„œ ì‚¬ìš©ëœ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ í—¤ë“œ ìˆ˜\n",
    "UNITS = 3072  # GPTì—ì„œ ì‚¬ìš©ëœ í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ í¬ê¸°\n",
    "DROPOUT = 0.1  # GPTì—ì„œ ì‚¬ìš©ëœ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    seq_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f1e3de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 19s 52ms/step - loss: 2.6307 - accuracy: 0.0469\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 15s 51ms/step - loss: 2.2900 - accuracy: 0.0544\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2587 - accuracy: 0.0544\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2496 - accuracy: 0.0544\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2454 - accuracy: 0.0544\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2418 - accuracy: 0.0544\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2399 - accuracy: 0.0544\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2365 - accuracy: 0.0544\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2337 - accuracy: 0.0544\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2354 - accuracy: 0.0545\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2386 - accuracy: 0.0544\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2393 - accuracy: 0.0544\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2370 - accuracy: 0.0544\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2314 - accuracy: 0.0545\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2271 - accuracy: 0.0545\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2239 - accuracy: 0.0545\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2295 - accuracy: 0.0545\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2301 - accuracy: 0.0546\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2327 - accuracy: 0.0545\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 15s 50ms/step - loss: 2.2277 - accuracy: 0.0545\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss_function, \n",
    "    metrics=[accuracy],\n",
    ")\n",
    "\n",
    "history = model.fit(dataset, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c338405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess(sentence)\n",
    "    sentence = tf.expand_dims(\n",
    "        SOS + tokenizer.encode(sentence), axis=0\n",
    "    )\n",
    "    output_sequence = tf.expand_dims(SOS, 0)\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if tf.equal(predicted_id, EOS[0]):\n",
    "            break\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "    return tf.squeeze(output_sequence, axis=0)\n",
    "\n",
    "\n",
    "def sentence_generation(sentence):    \n",
    "    prediction = decoder_inference(sentence)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    print(f\"ğŸ§‘ : {sentence}\")\n",
    "    print(f\"ğŸ¤– : {predicted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3bc38a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§‘ : ì•ˆë…•í•˜ì„¸ìš”!\n",
      "ğŸ¤– : \n"
     ]
    }
   ],
   "source": [
    "sentence_generation('ì•ˆë…•í•˜ì„¸ìš”!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f39f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§‘ : ì‹¤íŒ¨!!!!\n",
      "ğŸ¤– : \n"
     ]
    }
   ],
   "source": [
    "sentence_generation('ì‹¤íŒ¨!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1093ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
